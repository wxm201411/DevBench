本人长期使用AI编程，尤其是使用Trea国内版本（后称：Trae-CN）这IDE进行编程，相比其他IDE，Trae-CN的这个编程工具还是比较实在，一方面是全免费，至少目前是这样子的；另一方面国内顶级的大模型都是内置，无需本地部署或API调整，非常省心；最后也是最主要的是不用翻墙，合法合理可以放心使用。

近期Trae-CN把GLM-4.5，Kimi-k2和Qwen-3-Coder也内置进来了，就有了想对比和比较下它们各自在实际开发中的表现，也方便我后续针对不同的场景使用不同的大模型提供数据依据。

我打算从以下5个方面来对比和比较它们的表现：
-  1. 需求分析及需求说明文档生成
-  2. 原型理解及前端开发
-  3. API文档理解及后端开发
-  4. BUG修复
-  5. 综合项目开发

由于测试内容比较多，工作量大，因此我打算分期进行测试，每个阶段测试一个方面的表现。

本次测试的所有文档与代码都已经上传到github中：
`https://github.com/wxm201411/DevBench.git`

今天我先测试下需求分析及需求说明文档生成的表现。下面是测试内容与评估的标准

测试内容：

原始需求文档：`docs/requirement.md`:

```markdown
你的任务是根据下面的要求编写需求说明文档：

用户要求：开发一个校园二手图书交易商城，交易采用类似闲鱼APP的自提自付款方式

技术要求：

1. 前端：arco.design vue版本
2. 后端：FastApi+SQLModel
3. 数据库：MySQL9.3.0 连接信息：
   - 主机：localhost
   - 端口：3306
   - 用户名：root
   - 密码：root

输出要求：
- 遵循规范：EARS（简易需求语法方法）
```

评估标准：
1. **功能点覆盖率**（10分）
   - 自动检测文档中是否包含以下关键词：用户注册、登录、书籍搜索、分类浏览、购物车、结算支付、订单管理
   - 每个关键词匹配得1分，最高10分

2. **需求完整性**（10分）
   - 自动检测文档结构是否包含：功能需求、非功能需求、用户故事、验收标准
   - 每个部分存在得2.5分，最高10分

3. **描述清晰度**（10分）
   - 自动计算文档中明确可量化的需求数量
   - 每有一个可量化需求得1分，最高10分

而且测试也分两大部分进行：

 - 1、只给出原始需求与输出到指定文件的要求，让大模型自由发挥
 - 2、在上面的基础上使用需求生成器生成，需求生成器直接指定需求模板和编写步骤和要求，让大模型更规范输出，需求生成器的提示词见`docs/需求生成器.md`

# 大模型自由发挥测试结果

测试过程的截图见`images`目录下的图片，在此就不一一展示了。
生成的需求说明文档在`output`目录下

总结下测试过程中发现的问题：
Doubao-1.5-thinking-pro和Doubao-Seed-1.6直接修改了原需求文件，这个不应该被改的


最后的测试对比结果如下（按得分从高到低排序）：

| 需求文档 | 总分 | 得分率 |
| --- | --- | --- |
| GLM-4.5 | 40.6 | 54.13% |
| kimi-k2 | 38.1 | 50.8% |
| Qwen-3-Coder | 31.5 | 42.0% |
| Doubao-Seed-1.6 | 28.6 | 38.13% |
| DeepSeek-V3-0324 | 27.5 | 36.67% |
| Doubao-1.5-thinking-pro | 24.3 | 32.4% |
| DeepSeek-Reason | 19.4 | 25.87% |
| Doubao-1.5-pro | 14.0 | 18.67% |
| 平均分 | 28.0 | 37.33% |

# 需求生成器测试结果

测试过程的截图见`images_agent`目录下的图片，在此就不一一展示了。
生成的需求说明文档在`output_agent`目录下

总结下测试过程中发现的问题：

问题1：DeepSeek-Reason 和 DeepSeek-V3-0324既没有遵循需求生成器的指令先生成任务清单，也需要二次提示再能完成任务
问题2：Doubao-1.5-thinking-pro 直接修改了原需求文件，这个不应该被改的
问题3：GLM-4.5 重复读取原始需求文件，详见 `images_agent/error-2.png` 截图

最后的测试对比结果如下（按得分从高到低排序）：

| 需求文档 | 总分 | 得分率 |
| --- | --- | --- |
| Doubao-Seek-1.6 | 56.5 | 75.33% |
| Kimi-k2 | 53.6 | 71.47% |
| GLM-4.5 | 49.0 | 65.33% |
| Qwen-3-Coder | 46.1 | 61.47% |
| Doubao-1.5-pro | 42.5 | 56.67% |
| DeepSeek-V3-0324 | 41.3 | 55.07% |
| Doubao-1.5-thinking-pro | 40.6 | 54.13% |
| DeepSeek-Reason | 25.3 | 33.73% |
| 平均分 | 44.4 | 59.15% |

# 总结

上面两表格汇总相加，得到如下结果：

| 需求文档 | 总分 | 得分率 |
| --- | --- | --- |
| Kimi-k2 | 91.7 | 122.27% |
| GLM-4.5 | 89.6 | 119.46% |
| Doubao-Seek-1.6 | 85.1 | 113.46% |
| Qwen-3-Coder | 77.6 | 103.47% |
| DeepSeek-V3-0324 | 68.8 | 91.74% |
| Doubao-1.5-thinking-pro | 64.9 | 86.53% |
| Doubao-1.5-pro | 56.5 | 75.34% |
| DeepSeek-Reason | 44.7 | 59.60% |

在需求理解和需求说明文档编写方面，总的来说：
- Kimi-k2 在自由发辉与指定模板生成这两方面都比较稳定，应该是很适合文档编写的，网上看有人看它有时有点慢，但我基本感觉与其它模型速度相当。
- GLM-4.5 自由发辉比较好，也是写文档的不错选择，但在指定模板和指令遵循方面弱些，特别是处理文档方面可能会出现重复读取导致处理过慢的情况出现。
- Qwen-3-Coder在处理文档方面比较中规中矩，可能它更便于编程方面吧
- Doubao-Seek-1.6 无论是编程还是写文档，它是我之前大量使用的模型，这些表现也不错，只比近期推出的新模型低一点点

注意：以上是我在特定条件下的测试结果及个人主现意见，不代表各大模型的真实水平。

本测试的全部文档已经放到github中：
`https://github.com/wxm201411/DevBench.git`

欢迎大家查看，下一期将测试前端开发方面的名大模型的表现，欢迎先关注我，以便获取后续更新。